{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.15\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "id": "GiOeSOdrfLkK",
    "outputId": "bf4e6cdb-9a99-4eb3-ea19-a089d0d5f6cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\achta\\anaconda3\\lib\\site-packages (0.7.6)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from pdfplumber) (9.2.0)\n",
      "Requirement already satisfied: pdfminer.six==20221105 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from pdfplumber) (20221105)\n",
      "Requirement already satisfied: Wand>=0.6.10 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from pdfplumber) (0.6.10)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from pdfminer.six==20221105->pdfplumber) (38.0.4)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from pdfminer.six==20221105->pdfplumber) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\achta\\anaconda3\\lib\\site-packages (from pdfminer.six==20221105->pdfplumber) (4.4.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\achta\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gz2n9PK1fLhh",
    "outputId": "74ff153a-0b46-4f87-a1d0-e67d7c80e893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==3.5 in c:\\users\\achta\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\achta\\anaconda3\\lib\\site-packages (from transformers==3.5) (0.0.53)\n",
      "Requirement already satisfied: packaging in c:\\users\\achta\\anaconda3\\lib\\site-packages (from transformers==3.5) (21.3)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from transformers==3.5) (0.1.91)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from transformers==3.5) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\achta\\anaconda3\\lib\\site-packages (from transformers==3.5) (2.28.1)\n",
      "Requirement already satisfied: tokenizers==0.9.3 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from transformers==3.5) (0.9.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\achta\\anaconda3\\lib\\site-packages (from transformers==3.5) (3.19.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\achta\\anaconda3\\lib\\site-packages (from transformers==3.5) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from transformers==3.5) (2022.9.13)\n",
      "Requirement already satisfied: numpy in c:\\users\\achta\\anaconda3\\lib\\site-packages (from transformers==3.5) (1.21.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\achta\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==3.5) (0.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from packaging->transformers==3.5) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from requests->transformers==3.5) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from requests->transformers==3.5) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from requests->transformers==3.5) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from requests->transformers==3.5) (3.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\achta\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.5) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\achta\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.5) (8.1.3)\n",
      "Requirement already satisfied: six in c:\\users\\achta\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.5) (1.16.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\achta\\anaconda3\\lib\\site-packages (from click->sacremoses->transformers==3.5) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from importlib-metadata->click->sacremoses->transformers==3.5) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\achta\\anaconda3\\lib\\site-packages (from importlib-metadata->click->sacremoses->transformers==3.5) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jESyPQD3-4Uk",
    "outputId": "f1e6e960-49c2-4984-f691-6bc9d90195c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\achta\\\\Downloads'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rU0vvR9t_J_l",
    "outputId": "99567022-39e6-4fa2-f2a6-e248e6c85828"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'cdqa'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/cdqa-suite/cdqa.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zr9VFyB_SG_",
    "outputId": "cb54751c-de3f-4e4f-95fd-28220ebab768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achta\\Downloads\\cdqa\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/achta/Downloads/cdqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "s3q3gkFF_uoo",
    "outputId": "f1e85a0d-5ee2-40a5-b07f-5486fb6e53d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/achta/Downloads/cdqa\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting Flask==1.1.1\n",
      "  Using cached Flask-1.1.1-py2.py3-none-any.whl (94 kB)\n",
      "Collecting flask_cors==3.0.8\n",
      "  Using cached Flask_Cors-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting joblib==0.13.2\n",
      "  Using cached joblib-0.13.2-py2.py3-none-any.whl (278 kB)\n",
      "Collecting pandas==0.25.0\n",
      "  Using cached pandas-0.25.0-cp37-cp37m-win_amd64.whl (9.2 MB)\n",
      "Collecting prettytable==0.7.2\n",
      "  Using cached prettytable-0.7.2.zip (28 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers==2.1.1\n",
      "  Using cached transformers-2.1.1-py3-none-any.whl (311 kB)\n",
      "Collecting scikit_learn==0.21.2\n",
      "  Using cached scikit_learn-0.21.2-cp37-cp37m-win_amd64.whl (5.9 MB)\n",
      "Collecting tika==1.19\n",
      "  Using cached tika-1.19.tar.gz (24 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 1.1.0 Requires-Python >=3.8; 1.1.0rc1 Requires-Python >=3.8; 1.1.1 Requires-Python >=3.8; 1.1.2 Requires-Python >=3.8; 1.1.3 Requires-Python >=3.8; 1.2.0rc1 Requires-Python >=3.8; 1.4.0 Requires-Python >=3.8; 1.4.0rc0 Requires-Python >=3.8; 1.4.1 Requires-Python >=3.8; 1.4.2 Requires-Python >=3.8; 1.4.3 Requires-Python >=3.8; 1.4.4 Requires-Python >=3.8; 1.5.0 Requires-Python >=3.8; 1.5.0rc0 Requires-Python >=3.8; 1.5.1 Requires-Python >=3.8; 1.5.2 Requires-Python >=3.8\n",
      "ERROR: Could not find a version that satisfies the requirement torch==1.2.0 (from cdqa) (from versions: 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0)\n",
      "ERROR: No matching distribution found for torch==1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting python-dateutil\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting pytz\n",
      "  Using cached pytz-2022.6-py2.py3-none-any.whl (498 kB)\n",
      "Collecting six>=1.5\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, six, python-dateutil\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2022.6\n",
      "    Uninstalling pytz-2022.6:\n",
      "      Successfully uninstalled pytz-2022.6\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.6.1 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "tensorflow-gpu 2.6.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "rasa 3.0.4 requires attrs<21.3,>=19.3, but you have attrs 21.4.0 which is incompatible.\n",
      "rasa 3.0.4 requires joblib<1.1.0,>=0.15.1, but you have joblib 0.13.2 which is incompatible.\n",
      "rasa 3.0.4 requires jsonschema<3.3,>=3.2, but you have jsonschema 4.4.0 which is incompatible.\n",
      "rasa 3.0.4 requires packaging<21.0,>=20.0, but you have packaging 21.3 which is incompatible.\n",
      "rasa 3.0.4 requires pytz<2022.0,>=2019.1, but you have pytz 2022.6 which is incompatible.\n",
      "rasa 3.0.4 requires ruamel.yaml<0.17.0,>=0.16.5, but you have ruamel-yaml 0.17.21 which is incompatible.\n",
      "rasa 3.0.4 requires scikit-learn<0.25,>=0.22, but you have scikit-learn 0.21.2 which is incompatible.\n",
      "ont-bonito-cuda111 0.5.1 requires pandas==1.1.5, but you have pandas 0.25.0 which is incompatible.\n",
      "ont-bonito-cuda111 0.5.1 requires requests==2.25.1, but you have requests 2.28.0 which is incompatible.\n",
      "ont-bonito-cuda111 0.5.1 requires torch==1.10.0+cu111, but you have torch 1.2.0 which is incompatible.\n",
      "ont-bonito-cuda111 0.5.1 requires tqdm==4.31.1, but you have tqdm 4.32.2 which is incompatible.\n",
      "librosa 0.9.1 requires joblib>=0.14, but you have joblib 0.13.2 which is incompatible.\n",
      "grpcio-status 1.50.0 requires protobuf>=4.21.6, but you have protobuf 3.20.1 which is incompatible.\n",
      "google-api-core 2.10.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
      "datasets 1.6.2 requires huggingface-hub<0.1.0, but you have huggingface-hub 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed python-dateutil-2.8.2 pytz-2022.6 six-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dateutil pytz --force-reinstall --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "4Q00uKDBA3Ox"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "from cdqa.utils.converters import pdf_converter\n",
    "from cdqa.pipeline import QAPipeline\n",
    "from cdqa.utils.download import download_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tyy_HBGhB0xQ",
    "outputId": "4997a327-7aaa-4152-a5ef-c3c31365193b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading trained model...\n",
      "bert_qa.joblib already downloaded\n"
     ]
    }
   ],
   "source": [
    "download_model(model='bert-squad_1.1', dir='./models_CDQA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C9QCvte3B4ON",
    "outputId": "e3d290bb-e0f2-4457-c36e-2c9f4ac4ba3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_qa.joblib\n"
     ]
    }
   ],
   "source": [
    "!ls models_CDQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "frlettozB93x",
    "outputId": "f37f98c4-b99a-4beb-e392-4e28c65a07e7"
   },
   "outputs": [],
   "source": [
    "df = pdf_converter(directory_path=\"/srv/nas_data/atrabelsi/Documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e92R47JLC9HI",
    "outputId": "7b26c2ed-c88b-4531-bb0a-b72b245fcf18"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec  2 11:20:18 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:17:00.0 Off |                  N/A |\n",
      "| 30%   28C    P8    22W / 350W |   1768MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:65:00.0 Off |                  N/A |\n",
      "| 30%   28C    P8    17W / 350W |      3MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:B4:00.0 Off |                  N/A |\n",
      "| 30%   23C    P8    21W / 350W |      3MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1607928      C   ...conda/envs/PRT/bin/python     1765MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W89BNWwLDDS8",
    "outputId": "f9999d98-4c50-4a41-d48c-77c061e3b073"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QAPipeline(reader=BertQA(adam_epsilon=1e-08, bert_model='bert-base-uncased',\n",
       "                         do_lower_case=True, fp16=False,\n",
       "                         gradient_accumulation_steps=1, learning_rate=5e-05,\n",
       "                         local_rank=-1, loss_scale=0, max_answer_length=30,\n",
       "                         n_best_size=20, no_cuda=False,\n",
       "                         null_score_diff_threshold=0.0, num_train_epochs=3.0,\n",
       "                         output_dir=None, predict_batch_size=8, seed=42,\n",
       "                         server_ip='', server_po...size=8,\n",
       "                         verbose_logging=False, version_2_with_negative=False,\n",
       "                         warmup_proportion=0.1, warmup_steps=0),\n",
       "           retrieve_by_doc=False,\n",
       "           retriever=BM25Retriever(b=0.75, floor=None, k1=2.0, lowercase=True,\n",
       "                                   max_df=0.85, min_df=2, ngram_range=(1, 2),\n",
       "                                   preprocessor=None, stop_words='english',\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, top_n=20, verbose=False,\n",
       "                                   vocabulary=None))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdqa_pipeline = QAPipeline(reader='./models_CDQA/bert_qa.joblib')\n",
    "\n",
    "cdqa_pipeline.fit_retriever(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXSwsVh0DI4R",
    "outputId": "c9a3238d-e272-456a-d549-fbc49a80fef9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/srv/nas_data/atrabelsi/models_CDQA/bert_qa_custom.joblib']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(cdqa_pipeline, '/srv/nas_data/atrabelsi/models_CDQA/bert_qa_custom.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrJE5CxxDMq3",
    "outputId": "2ff104ce-c7bd-49df-a0bd-ac055bf8b65f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QAPipeline(reader=BertQA(adam_epsilon=1e-08, bert_model='bert-base-uncased',\n",
       "                         do_lower_case=True, fp16=False,\n",
       "                         gradient_accumulation_steps=1, learning_rate=5e-05,\n",
       "                         local_rank=-1, loss_scale=0, max_answer_length=30,\n",
       "                         n_best_size=20, no_cuda=False,\n",
       "                         null_score_diff_threshold=0.0, num_train_epochs=3.0,\n",
       "                         output_dir=None, predict_batch_size=8, seed=42,\n",
       "                         server_ip='', server_po...size=8,\n",
       "                         verbose_logging=False, version_2_with_negative=False,\n",
       "                         warmup_proportion=0.1, warmup_steps=0),\n",
       "           retrieve_by_doc=False,\n",
       "           retriever=BM25Retriever(b=0.75, floor=None, k1=2.0, lowercase=True,\n",
       "                                   max_df=0.85, min_df=2, ngram_range=(1, 2),\n",
       "                                   preprocessor=None, stop_words='english',\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, top_n=20, verbose=False,\n",
       "                                   vocabulary=None))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdqa_pipeline=joblib.load('/srv/nas_data/atrabelsi/models_CDQA/bert_qa_custom.joblib')\n",
    "cdqa_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0.130\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; print(torch.version.cuda)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QAPipeline(reader=BertQA(adam_epsilon=1e-08, bert_model='bert-base-uncased',\n",
       "                         do_lower_case=True, fp16=False,\n",
       "                         gradient_accumulation_steps=1, learning_rate=5e-05,\n",
       "                         local_rank=-1, loss_scale=0, max_answer_length=30,\n",
       "                         n_best_size=20, no_cuda=False,\n",
       "                         null_score_diff_threshold=0.0, num_train_epochs=3.0,\n",
       "                         output_dir=None, predict_batch_size=8, seed=42,\n",
       "                         server_ip='', server_po...size=8,\n",
       "                         verbose_logging=False, version_2_with_negative=False,\n",
       "                         warmup_proportion=0.1, warmup_steps=0),\n",
       "           retrieve_by_doc=False,\n",
       "           retriever=BM25Retriever(b=0.75, floor=None, k1=2.0, lowercase=True,\n",
       "                                   max_df=0.85, min_df=2, ngram_range=(1, 2),\n",
       "                                   preprocessor=None, stop_words='english',\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None, top_n=20, verbose=False,\n",
       "                                   vocabulary=None))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send it to CPU because i have got a problem with GPU\n",
    "cdqa_pipeline.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "sYkgi52gDVGA",
    "outputId": "1fc8ecc8-7ebe-4a11-c836-20f01addc529"
   },
   "outputs": [],
   "source": [
    "query = 'How much is increase in operating cash flow?'\n",
    "prediction = cdqa_pipeline.predict(query, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMzl1lp5DaNs",
    "outputId": "7a397b67-cf03-4385-b55f-0ec3e30e2edc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('16 %',\n",
       "  'AMZN-Q1-2020-Earnings-Release',\n",
       "  'Cash Flows and Shares        Operating cash flow -- trailing twelve months (TTM) $ 30,723 $ 34,360 $ 36,029 $ 35,332 $ 38,514 $ 39,732 16 %Operating cash flow -- TTM Y/Y growth 67% 89 % 65% 33 % 25 % 16 % N/A ',\n",
       "  16.82219907456501),\n",
       " ('25 %',\n",
       "  'Amazon-Q4-2019-Earnings-Release',\n",
       "  'Cash Flows and Shares        Operating cash flow -- trailing twelve months (TTM) $ 26,604 $ 30,723 $ 34,360 $ 36,029 $ 35,332 $ 38,514 25 %Operating cash flow -- TTM Y/Y growth 57% 67 % 89% 65% 33 % 25% N/A ',\n",
       "  16.391582435435613),\n",
       " ('33 %',\n",
       "  'Q3-2019-Amazon-Financial-Results',\n",
       "  'Cash Flows and Shares        Operating cash flow -- trailing twelve months (TTM) $ 21,793 $ 26,604 $ 30,723 $ 34,360 $ 36,029 $ 35,332 33 %Operating cash flow -- TTM Y/Y growth 22% 57 % 67% 89% 65 % 33 % N/A ',\n",
       "  16.184274119204836)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "ueXr03OKDkzd"
   },
   "outputs": [],
   "source": [
    "query = 'What is latest earnings per share?'\n",
    "prediction = cdqa_pipeline.predict(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TAdXHM8hERs1",
    "outputId": "9e66ebed-71c0-445b-8e26-24afede63c74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('$ 7.09 $ 5.01',\n",
       " 'AMZN-Q1-2020-Earnings-Release',\n",
       " 'Diluted earnings per share $ 7.09  $ 5.01 ',\n",
       " 13.915514359397722)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "i_0my_XODwQt"
   },
   "outputs": [],
   "source": [
    "query = 'How many jobs are created in 2020?'\n",
    "prediction = cdqa_pipeline.predict(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ujEJrKGUEjt3",
    "outputId": "c5a27714-484a-4e0b-d73b-672366b088dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: How many jobs are created in 2020?\n",
      "answer: 1.1 million\n",
      "title: Q2-2020-Amazon-Earnings-Release\n",
      "paragraph: Empowering Small and Medium-Sized Businesses• Amazon released its 2020 Small and Medium-Sized Business (SMB) Impact Report, highlighting how SMBs selling in its U.S. store have sold more than 3.4 billion products in the past year and created an estimated 1.1 million jobs.• In partnership with the British small business support network “Enterprise Nation,” Amazon launched the Amazon Small Business Accelerator, which aims to support more than 200,000 small businesses across the U.K. negatively impacted by the COVID-19 crisis. Amazon hosted a week-long boot camp in the U.K. to help 1,000 offline businesses get online, and offered free services, AWS credits, training, and support.• In the U.K., Amazon worked with the British Chambers of Commerce to give up to 1,000 businesses tours of Amazon fulfillment centers, helping other companies learn from the safety measures Amazon has put in place within its own operations so the businesses can re-open safely and kick-start the economy.• Amazon in Japan launched Global Selling to allow Japanese sellers to reach new customers across 16 Amazon sites worldwide. • Amazon in India announced plans to help digitally enable micro, small, and medium businesses across the country as part of a $1 billion investment pledge. Amazon launched Local Shops on Amazon.in, offering shopkeepers and retailers with physical stores the ability to register to serve more customers from their local areas. Since launch, more than 11,000 sellers have enrolled in the program. In addition, Amazon introduced seller registration and account management services in Hindi to help businesses overcome language barriers. Since launch, more than 10,000 sellers have used Hindi to register on Amazon.in. \n"
     ]
    }
   ],
   "source": [
    "print('query: {}'.format(query))\n",
    "print('answer: {}'.format(prediction[0]))\n",
    "print('title: {}'.format(prediction[1]))\n",
    "print('paragraph: {}'.format(prediction[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "CzdQXUdyEkrq"
   },
   "outputs": [],
   "source": [
    "query = 'General Availability of which AWS services were announced?'\n",
    "prediction = cdqa_pipeline.predict(query, n_predictions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cS3Z9b89ErMe",
    "outputId": "b6535ca8-7c9e-47cb-b946-dbd510764ca6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Amazon Detective',\n",
       "  'AMZN-Q1-2020-Earnings-Release',\n",
       "  'Amazon Web Services •  AWS announced the opening of the AWS Europe (Milan) and AWS Africa (Cape Town) Regions. AWS now spans 76 Availability Zones within 24 geographic regions, with announced plans for nine more Availability Zones and three more AWS Regions in Indonesia, Japan, and Spain.                                                        •  AWS announced the general availability of Amazon Detective, a security service that makes it easy for customers to conduct faster and more efficient investigations into security issues across their AWS workloads. Amazon Detective automatically collects log data from a customer’s resources and uses machine learning, statistical analysis, and graph theory to build interactive visualizations that help customers analyze, investigate, and quickly identify the root cause of potential security issues or suspicious activities.  •  AWS announced the general availability of Amazon Keyspaces (for Apache Cassandra), a scalable, highly available, and fully managed database service for Cassandra workloads. Amazon Keyspaces supports the same application code, ',\n",
       "  19.345390511291928),\n",
       " ('Amazon Augmented Artificial Intelligence',\n",
       "  'AMZN-Q1-2020-Earnings-Release',\n",
       "  'Marketo, ServiceNow, Trend Micro, and Zendesk — without writing custom integration code. Amazon AppFlow also works with AWS PrivateLink to route data flows through the AWS network instead of over the public Internet to provide even stronger data privacy and security. With Amazon AppFlow, customers can bring together and manage petabytes, even exabytes, of data spread across all of their applications without having to develop custom connectors or manage underlying API and network connectivity. •  AWS announced the general availability of Amazon Augmented Artificial Intelligence (Amazon A2I), a fully managed service that makes it easy to add human review to machine learning predictions to enhance model and application accuracy by continuously identifying and improving low confidence predictions. Human review for model predictions can be added to new or existing applications using reviewers from Mechanical Turk, third party vendors, or a customer’s own employees. •  AWS helped power the NFL’s first ever remote draft — the most watched ever, reaching more than 55 million viewers total. Through all seven rounds of the three-day event, AWS ensured that over 100 live feeds ran successfully, creating a seamless experience for the NFL, teams, coaches, players and their families, fans, and everyone watching.  ',\n",
       "  14.319974520581688),\n",
       " ('AWS Outposts',\n",
       "  'Amazon-Q4-2019-Earnings-Release',\n",
       "  '550,000 sellers on the Amazon India marketplace, more than 60,000 Indian manufacturers and brands are exporting their “Make in India” products to customers worldwide on Amazon, and we expect our new $1 billion investment to enable $10 billion in cumulative Indian exports by 2025.  •  Since launching amazon.in in 2013, Amazon has created more than 700,000 direct and indirect jobs in India. In January, Amazon announced plans to create an additional one million jobs in India by 2025, with continued investments in technology, infrastructure, and logistics. Since 2014, Amazon has grown its employee base more than four times, and last year inaugurated its new campus building in Hyderabad — Amazon’s first fully-owned campus outside the United States and the largest building globally in terms of employees and space. •  Amazon India announced it will have 10,000 electric vehicles in its delivery fleet by 2025. This investment is part of Amazon’s recent co-founding of The Climate Pledge, a commitment to meet the Paris Agreement 10 years early by achieving net zero carbon emissions by 2040. •  Amazon Business launched in Canada and now serves commercial and public sector organizations of all sizes in Canada and eight other countries, including the U.S., U.K., Germany, France, Italy, Spain, Japan, and India. Amazon Business also launched Business Prime in Canada. Business Prime offers member-only benefits to help save time and money, in addition to unlimited fast, free shipping on eligible items for every member on the account. •  Amazon Fashion produced its first holiday catalog featuring influencers Louise Roe, Ayana Iman Gibbs, Tylynn Nguyen, and their families in our favorite holiday looks. Additionally, Amazon Fashion and Amazon Home partnered with Refinery29 to open a holiday pop-up shop in New York City showcasing selection for the whole family. •  Amazon Fashion continues to increase its assortment with the introduction of new brands, such as Tissot, Swatch, and Vineyard Vines, as well as the expansion of the Amazon brand, Goodthreads, which now offers women’s styles. •  In 2019, authors earned more than $300 million from the Kindle Direct Publishing (KDP) Select Global Fund, totaling more than $1.1 billion since the launch of Kindle Unlimited. Millions of independent authors have self-published millions of books through KDP since launching the service in 2007. Additionally, thousands of independent authors earned more than $50,000, with more than a thousand surpassing $100,000 in royalties in 2019 through KDP. •  Independent third-party sellers — mostly small and medium-sized businesses — sold more than a billion items during the holiday season, including more than 100 million items shipped with Prime Free One-Day Delivery. Amazon also announced nine winners of the Small Business Spotlight Awards in the U.K., Germany, and the U.S., including Small Business of the Year winners Gamely Games in the U.K., Snocks in Germany, and nutpods in the U.S.  •  Amazon was recognized by the Drucker Institute as the #1 Best-Managed Company of 2019, based on a study that measures corporate performance across customer satisfaction, employee engagement and development, innovation, social responsibility, and financial strength. Amazon received a perfect score on the Human Rights Campaign’s 2020 Corporate Equality Index and the designation of being a “Best Place to Work for LGBTQ Equality.”  Employers earning top ratings took steps to ensure greater equity for lesbian, gay, bisexual, transgender, and queer (LGBTQ) workers and their families in the form of comprehensive policies, benefits, and practices. •  Amazon announced six new renewable energy projects across the U.S. and Europe that will support Amazon’s commitment to The Climate Pledge to be net zero carbon by 2040 and supply renewable energy for the company’s fulfillment network and Amazon Web Services (AWS) data centers. To date, Amazon has launched over 70 renewable energy projects that will provide over 1,900 MW of renewable capacity and are projected to deliver more than 5.3 million MWh of renewable energy annually. •  This holiday season, Amazon surprised hundreds of charities across the U.S. by donating hundreds of thousands of items from their AmazonSmile Charity Lists, including more than 5,000 blankets, sheets, and pillows; more than 30,000 toys and educational items for kids; more than 40,000 apparel items; and more than 60,000 pantry items. •  AWS announced several significant new customer commitments and migrations during the quarter spanning major industries, including finance with Western Union, FINRA CAT, LLC, a subsidiary of FINRA (Financial Industry Regulatory Authority), and Klarna, a leading global payments provider and bank; media with Fox Corporation, and ProSiebenSat.1 Media SE, Europe’s leading satellite and cable broadcaster; sports with a new player health and safety initiative with the NFL, the Seattle Seahawks, Bundesliga, Germany’s premier football league, and Formula One Group; energy with BP; pharmaceuticals with Novartis; and hospitality with Best Western Hotels & Resorts, among others.  •  AWS announced three Arm-based instances (M6g, C6g, R6g) powered by AWS’s new Graviton2 processors, that deliver up to 40% better price and performance than current x86 processor-based instances. These new Arm-based instances are powered by the AWS Nitro System, a combination of dedicated hardware and lightweight hypervisor, enabling faster innovation and enhanced security for customers at a much lower cost.  •  AWS announced the general availability of AWS Outposts, a fully-managed service that extends AWS infrastructure and services to virtually any data center, co-location space, or on-premises facility. AWS Outposts offers customers the same AWS hardware infrastructure, services, APIs, and tools to build and run applications on premises and in the cloud for a truly consistent hybrid experience. AWS compute, storage, database, and other services run locally on    ',\n",
       "  13.80761293041802),\n",
       " ('AWS Local Zones',\n",
       "  'Amazon-Q4-2019-Earnings-Release',\n",
       "  'Outposts, and customers can access the full range of AWS services available in the region to build, manage, and scale on-premises applications using familiar AWS services and tools. •  AWS announced AWS Local Zones, a new type of AWS infrastructure deployment that places AWS compute, storage, database, and other select services closer to large population, industry, and IT centers where no AWS Region exists today. With the opening of the first AWS Local Zone in Los Angeles (LA), developers will have the ability to deploy applications that require single-digit millisecond latencies to end-users also in LA. AWS Local Zone customers will be able to use their compute, storage, database, and other select services locally in LA, while also being able to seamlessly connect back to the rest of their workloads running in the AWS U.S. West (Oregon) Region or other AWS Regions a customer may be using.  •  AWS announced AWS Wavelength, which enables developers to build applications that deliver single-digit millisecond latencies to mobile devices and users by deploying AWS compute and storage at the edge of the 5G network. With AWS Wavelength, developers can serve use cases that require ultra-low latency like machine learning inference at the edge, autonomous industrial equipment, smart cars and cities, Internet of Things (IoT), and Augmented and Virtual Reality. AWS is partnering with Verizon on making AWS Wavelength available across the U.S., and is collaborating with other leading telecommunications companies, including Vodafone and SK Telecom, to launch AWS Wavelength across Europe and South Korea in 2020, with more global partners coming soon. •  AWS announced six new capabilities for Amazon SageMaker, a fully-managed service that removes the heavy lifting from each step of the machine learning process. Amazon SageMaker Studio is the first fully integrated development environment for machine learning that makes it easier for developers to build, debug, train, deploy, monitor, and operate custom machine learning models; Amazon SageMaker Notebooks allows developers to spin up elastic machine learning notebooks in seconds, and automates the process of sharing notebooks with a single-click; Amazon SageMaker Experiments helps developers visualize and compare machine learning model iterations, training parameters, and outcomes; Amazon SageMaker Autopilot allows developers to submit simple data in CSV files and have machine learning models automatically generated, with full visibility to how the models are created so developers can evolve them over time; Amazon SageMaker Debugger provides real-time monitoring for machine learning models to improve predictive accuracy, reduce training times, and facilitate greater explainability; and Amazon SageMaker Model Monitor detects concept drift to discover when the performance of a model running in production begins to deviate from the original trained model.  •  AWS announced five new artificial intelligence (AI) services designed to put machine learning in the hands of more developers — with no machine learning experience required. Amazon Kendra reinvents enterprise search by using natural language processing and other machine learning techniques to unite multiple data silos inside an enterprise and consistently provide high-quality results to common queries instead of a random list of links in response to keyword queries; Amazon CodeGuru helps software developers automate code reviews and identify an application’s most expensive lines of code; Amazon Fraud Detector helps businesses identify online identity and payment fraud in real time, based on the same technology developed for Amazon.com; Amazon Transcribe Medical offers healthcare providers highly accurate, real-time speech-to-text transcription so they can focus on patient care; and Amazon Augmented Artificial Intelligence (A2I) helps developers validate machine learning predictions through human confirmation. •  AWS announced a set of machine learning-powered analytics capabilities for Amazon Connect called Contact Lens, which make it easier for businesses to identify customer issues and trends, search call and chat transcripts, and improve agent performance. Amazon Connect offers customers a fully-managed cloud contact center service. Now with Contact Lens, Amazon Connect customers have the ability to understand the sentiment, trends, and compliance of their own customer conversations to improve the experience and identify crucial feedback, with no machine learning experience required. •  AWS announced significant new analytics capabilities in Redshift that provide an order of magnitude better query performance, deliver greater flexibility, and help customers embrace data at scale. Amazon Redshift RA3 instances allow customers to optimize their data warehouse by scaling and paying for compute and storage independently, so they can choose the number of instances they need based on their data warehousing workload’s performance requirements, and only pay for the managed storage that they use; AQUA (Advanced Query Accelerator) for Amazon Redshift is a new distributed and hardware-accelerated cache that brings compute to the storage layer, so data doesn’t have to move back and forth between the two, giving customers up to 10x better query performance than other cloud data warehouse provider; Amazon Redshift Data Lake Export allows customers to export data directly from Amazon Redshift to Amazon S3 in an open data format optimized for analytics; and Amazon Redshift Federated Query lets customers analyze data across their Amazon Redshift data warehouse, Amazon Simple Storage Service (S3) Data Lake, and Amazon RDS and Aurora databases.  •  AWS announced a new innovative highly-scalable, cost-saving warm storage tier for Amazon Elasticsearch Service called UltraWarm that makes it easier for customers to retain any amount of current and historical log data at up to    one-tenth the current cost and is 80% less than the cost of warm-tier storage from other managed Elasticsearch offerings. •  AWS announced Amazon Managed (Apache) Cassandra Service, a scalable, highly available, and fully-managed database service that supports Cassandra workloads. Developers can use the same Cassandra application code, Apache 2.0 licensed drivers, and tools as they do today to run, manage, and scale workloads on Amazon Managed Cassandra Service without having to worry about managing the underlying infrastructure. And, because it’s serverless, it also removes the need to provision, configure, and operate large Cassandra clusters, manually add or remove nodes, and rebalance partitions as traffic scales up or down.  •  AWS announced three new services and capabilities that make it easier for customers to build and operate securely. Amazon Detective analyzes trillions of data points, using machine learning, statistical analysis, and graph theory to make it easier to visualize and conduct faster and more efficient security investigations; AWS IAM Access Analyzer makes it simple for security teams and administrators to audit resource policies for unintended access by analyzing hundreds or even thousands of policies across a customer’s environment in seconds, and delivering detailed findings about resources that are accessible from outside the account; and AWS Nitro Enclaves is a new Amazon EC2 capability that makes it easy for customers in healthcare, financial services, energy, media and entertainment, and other data-intensive industries to process highly sensitive data, like personally identifiable information and intellectual property on their compute instances, particularly from internal threats within their own accounts.  •  AWS announced that customers can start using AWS Fargate for Amazon Elastic Kubernetes Service (EKS), making it easier for customers to run Kubernetes applications on AWS. AWS Fargate, which provides serverless computing for containers, has substantially changed the way developers manage and deploy their containers. Launched two years ago to work with Amazon Elastic Container Service (ECS), AWS Fargate has been broadly requested by Kubernetes customers. Now, with AWS Fargate for Amazon EKS, customers can run Kubernetes-based applications on AWS without the need to manage servers and clusters.  •  AWS announced three key initiatives as a part of its plans to help advance quantum computing technologies. Amazon Braket is a new, fully-managed AWS service that enables scientists, researchers, and developers to begin experimenting with computers from quantum hardware providers (including D-Wave, IonQ, and Rigetti) in a single place; AWS Center for Quantum Computing will bring together quantum computing experts from Amazon, the California Institute of Technology (Caltech), and other top academic research institutions to collaborate on the research and development of new quantum computing technologies; and the Amazon Quantum Solutions Lab connects customers with quantum computing experts from Amazon and its partners to develop internal expertise aimed at identifying practical uses of quantum computing, and accelerating the development of quantum applications with meaningful impact.  ',\n",
       "  11.570854597254657),\n",
       " ('Amazon AppFlow',\n",
       "  'AMZN-Q1-2020-Earnings-Release',\n",
       "  'Keyspaces, customers can easily migrate on-premises Cassandra workloads to the cloud, without having to provision, configure, and operate servers or large Cassandra clusters, or needing to manually add or remove nodes or rebalance partitions as traffic scales up or down.    •  AWS announced Amazon AppFlow, a fully managed service that provides an easy, secure way for customers to create and automate bidirectional data flows between AWS and SaaS applications — such as Salesforce, Slack, Infor Nexus, ',\n",
       "  9.840711473434734)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "6ioB81MBFAKD"
   },
   "outputs": [],
   "source": [
    "query = 'What is the impact of COVID on business?'\n",
    "prediction = cdqa_pipeline.predict(query, n_predictions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2zQ5MQ1FEia",
    "outputId": "d0ae02b0-c500-4065-9d1e-7918520c1135"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lessen the impact that COVID-19 has on families, communities, and businesses',\n",
       "  'AMZN-Q1-2020-Earnings-Release',\n",
       "  'Amazon Small Business Academy to help entrepreneurs learn how to build their businesses online. •  AWS is helping healthcare workers, medical researchers, scientists, and public health officials working to understand and fight COVID-19 by providing a centralized repository of curated, up-to-date, pre-processed, and publicly-readable datasets focused on the spread and characteristics of the virus. The AWS COVID-19 data lake, which includes data sets from Johns Hopkins University, Definitive Healthcare, Carnegie Mellon’s Delphi Research Group, and other sources, is available for anyone researching, tracking, deploying vital resources, or developing other helpful solutions and applications to combat COVID-19.   •  AWS is supporting the White House’s COVID-19 High Performance Computing Consortium, providing computing resources to advance research on diagnosis, treatment, and vaccines. •  Customers are using AWS to lessen the impact that COVID-19 has on families, communities, and businesses. Examples include: •  The New York City COVID-19 Rapid Response Coalition is using a conversational agent, which is running on AWS, to enable at-risk, elderly New Yorkers to receive accurate, timely information about medical needs.  •  The Los Angeles Unified School District is using AWS to power a new call center that is fielding IT questions, providing remote support, and enabling staff to answer calls around remote learning for 700,000 students.  •  Volunteer Surge, a nonprofit consortium, is running its online training platform on AWS to recruit, train, and deploy one million volunteer health workers. •  The World Health Organization is using AWS to build large-scale data lakes, aggregate epidemiological country data, rapidly translate medical training videos into different languages, and help global healthcare workers better treat patients. •  Cerner, a global healthcare technology company, is compiling de-identified patient data to help COVID-19 researchers and is leveraging AWS to secure and store critical information. The data is available free of charge and will support research, vaccine development, and new treatment options, allowing organizations to share information and accelerate understanding of the virus.  •  In England, the National Health Service is using AWS to analyze hospital occupancy levels, emergency room capacity, and patient wait times in order to help decide where best to allocate resources. •  AWS is helping Kentucky and West Virginia authorities address the surge in call volumes to unemployment call centers by transitioning from legacy technology that often required agents to work in the states’ offices to ',\n",
       "  4.910572448236516),\n",
       " ('surge in unemployment insurance applications',\n",
       "  'AMZN-Q1-2020-Earnings-Release',\n",
       "  'Amazon Connect which enables staff to work remotely via the AWS Cloud.  •  Origin Energy, Australia’s largest integrated energy retailer, is using Amazon Connect to operate its cloud-based contact center and successfully manage inbound calls by enabling their contact center agents to work remotely during the COVID-19 crisis. •  AWS is helping the state of Rhode Island address a surge in unemployment insurance applications — 10 times the volume their legacy system could typically handle — by powering the Department of Labor and ',\n",
       "  3.699655702091447),\n",
       " ('no additional business acquisitions, investments, restructurings, or legal settlements are concluded.',\n",
       "  'AMZN-Q1-2020-Earnings-Release',\n",
       "  'Second Quarter 2020 Guidance •  Net sales are expected to be between $75.0 billion and $81.0 billion, or to grow between 18% and 28% compared with second quarter 2019. This guidance anticipates an unfavorable impact of approximately 70 basis points from foreign exchange rates. •  Operating income (loss) is expected to be between $(1.5) billion and $1.5 billion, compared with $3.1 billion in second quarter 2019. This guidance assumes approximately $4.0 billion of costs related to COVID-19. •  This guidance assumes, among other things, that no additional business acquisitions, investments, restructurings, or legal settlements are concluded.  ',\n",
       "  3.677764514551123),\n",
       " ('relief',\n",
       "  'AMZN-Q1-2020-Earnings-Release',\n",
       "  'What we are doing for communities •  Using our network of Amazon Flex drivers, we’re partnering with food banks across the U.S. to donate delivery services of groceries to serve six million meals through the end of June. We’ve delivered 427,000 pounds of groceries, representing 336,000 meals.  •  We’ve partnered with a Seattle catering company to distribute more than 73,000 meals to over 2,700 elderly and medically-vulnerable residents in Seattle and King County during the outbreak.  •  We’re donating $5 million in Amazon devices to support healthcare workers, patients, schools, teachers, and communities around the globe that have been impacted by COVID-19. •  We’ve provided $10 million in funding to provide relief to more than 800 small businesses in Seattle and the Puget Sound through cash grants and free rent as part of our Neighborhood Small Business Relief Fund.   •  We have donated 12,200 laptops to students across the U.S. and are making online computer science resources, including exam prep, available at no cost to students, parents, and teachers through Amazon Future Engineer.  •  We committed more than €21 million to relief organizations across Europe to support those most affected by COVID-19. •  In partnership with the United States Department of Agriculture we’ve expanded the list of states where Supplemental Nutrition Assistance Program (SNAP) benefits can be used online, which now includes Alabama, Arizona, California, ',\n",
       "  2.385920634816745),\n",
       " ('favorable',\n",
       "  'Amazon-Q4-2019-Earnings-Release',\n",
       "  'Operating income $ 3,724 $ 3,786 $ 4,420 $ 3,084 $ 3,157 $ 3,879 2 %F/X impact -- favorable (unfavorable) $ 90 $ 123 $ 84 $ 58 $ 22 $ 16 N/A Operating income -- Y/Y growth (decline), excluding F/X 948% 72 % 125% 1% (16)% 2% N/A ',\n",
       "  1.6444199816241098)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install ipywidgets\n",
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "_JxfGxJBfLbi"
   },
   "outputs": [],
   "source": [
    "# retriever_score_weight\n",
    "#Evaluate the pipeline object:\n",
    "from cdqa.utils.evaluation import evaluate_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tqdm/_tqdm_notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIntProgress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# No total? Show info style bar with no progress tqdm status\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IntProgress' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1607928/3649313553.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcdqa_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/srv/nas_data/atrabelsi/test.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/nas_data/atrabelsi/cdqa/cdqa/utils/evaluation.py\u001b[0m in \u001b[0;36mevaluate_pipeline\u001b[0;34m(cdqa_pipeline, annotated_json, output_dir, n_predictions, verbose)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_queries_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mall_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pipeline_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcdqa_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/nas_data/atrabelsi/cdqa/cdqa/utils/evaluation.py\u001b[0m in \u001b[0;36m_pipeline_predictions\u001b[0;34m(cdqa_pipeline, queries, n_predictions)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mall_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_predictions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mall_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcdqa_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tqdm/_tqdm_notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         self.sp = self.status_printer(\n\u001b[0;32m--> 214\u001b[0;31m             self.fp, total, self.desc, self.ncols)\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# trick to place description before the bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tqdm/_tqdm_notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# #187 #451 #558\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             raise ImportError(\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0;34m\"IntProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \"/user_install.html\")\n",
      "\u001b[0;31mImportError\u001b[0m: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(cdqa_pipeline, '/srv/nas_data/atrabelsi/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate The reader\n",
    "from cdqa.utils.evaluation import evaluate_reader\n",
    "evaluate_reader(cdqa_pipeline, 'path-to-annotated-dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Question Answering System Using CDQA on Multiple Pdf Files.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
